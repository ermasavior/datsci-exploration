{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Pre-processing for Text Data\n",
    "1. **Tokenization**: given a text, this will separate it into individual words.\n",
    "2. **Normalization**: convert text into all lowercase, spelling mistake correction, etc. \n",
    "3. **Cleaning**: remove unwanted parts, e.g., punctuation, stop words, etc.\n",
    "4. **Lemmatization**/**stemming**: convert individual words to the corresponding 'root word'. There is a difference between 'lemmatization' & 'stemming', you may check in some references if you want to know further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens1:\n",
      " ['After', 'watching', 'two', 'hours', 'non', 'stop', ',', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '#', 'brilliant', '.']\n",
      "tokens2:\n",
      " ['Foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', ',', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '#', 'notrecommended', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = \"After watching two hours non stop, \\\n",
    "        he says that the film is really fantastic #brilliant.\"\n",
    "text2 = \"Foods sold there are little bit pricy, \\\n",
    "        meanwhile the taste is not delicious #notrecommended.\"\n",
    "\n",
    "tokens1 = word_tokenize(text1)\n",
    "print(\"tokens1:\\n\", tokens1)\n",
    "\n",
    "tokens2 = word_tokenize(text2)\n",
    "print(\"tokens2:\\n\", tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "In this block of code, we try one of normalization processes: converting to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalised_words1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', ',', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '#', 'brilliant', '.']\n",
      "normalised_words2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', ',', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '#', 'notrecommended', '.']\n"
     ]
    }
   ],
   "source": [
    "normalized_words1 = [w.lower() for w in tokens1]\n",
    "print(\"normalised_words1:\\n\", normalized_words1)\n",
    "\n",
    "normalized_words2 = [w.lower() for w in tokens2]\n",
    "print(\"normalised_words2:\\n\", normalized_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning 01: remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punc_removed1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', '', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '', 'brilliant', '']\n",
      "punc_removed2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', '', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '', 'notrecommended', '']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "punc_removed1 = [w.translate(table) for w in normalized_words1]\n",
    "print(\"punc_removed1:\\n\", punc_removed1)\n",
    "\n",
    "punc_removed2 = [w.translate(table) for w in normalized_words2]\n",
    "print(\"punc_removed2:\\n\", punc_removed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning 02: remove not alphabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isalpha_words1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', 'brilliant']\n",
      "isalpha_words2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "isalpha_words1 = [word for word in punc_removed1 if word.isalpha()]\n",
    "print(\"isalpha_words1:\\n\", isalpha_words1)\n",
    "\n",
    "isalpha_words2 = [word for word in punc_removed2 if word.isalpha()]\n",
    "print(\"isalpha_words2:\\n\", isalpha_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning 03: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopWords_removed1:\n",
      " ['watching', 'two', 'hours', 'non', 'stop', 'says', 'film', 'really', 'fantastic', 'brilliant']\n",
      "stopWords_removed2:\n",
      " ['foods', 'sold', 'little', 'bit', 'pricy', 'meanwhile', 'taste', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(\"stop_words:\\n\", stop_words)\n",
    "\n",
    "stopWords_removed1 = [w for w in isalpha_words1 if not w in stop_words]\n",
    "print(\"stopWords_removed1:\\n\", stopWords_removed1)\n",
    "\n",
    "stopWords_removed2 = [w for w in isalpha_words2 if not w in stop_words]\n",
    "print(\"stopWords_removed2:\\n\", stopWords_removed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed_word1:\n",
      " ['watch', 'two', 'hour', 'non', 'stop', 'say', 'film', 'realli', 'fantast', 'brilliant']\n",
      "stemmed_word2:\n",
      " ['food', 'sold', 'littl', 'bit', 'prici', 'meanwhil', 'tast', 'delici', 'notrecommend']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_word1 = [ps.stem(w) for w in stopWords_removed1]\n",
    "print(\"stemmed_word1:\\n\", stemmed_word1)\n",
    "\n",
    "stemmed_word2 = [ps.stem(w) for w in stopWords_removed2]\n",
    "print(\"stemmed_word2:\\n\", stemmed_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_words1:\n",
      " ['watching', 'two', 'hour', 'non', 'stop', 'say', 'film', 'really', 'fantastic', 'brilliant']\n",
      "lemmatized_words2:\n",
      " ['food', 'sold', 'little', 'bit', 'pricy', 'meanwhile', 'taste', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words1 = [lemmatizer.lemmatize(w) for w in stopWords_removed1]\n",
    "print(\"lemmatized_words1:\\n\", lemmatized_words1)\n",
    "\n",
    "lemmatized_words2 = [lemmatizer.lemmatize(w) for w in stopWords_removed2]\n",
    "print(\"lemmatized_words2:\\n\", lemmatized_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Converting Preprocessed Text into Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_features of text1:\n",
      " [ 0.          0.31622777  0.          0.31622777  0.31622777  0.\n",
      "  0.31622777  0.          0.          0.31622777  0.          0.\n",
      "  0.31622777  0.31622777  0.          0.31622777  0.          0.31622777\n",
      "  0.31622777] ; shape: (19,)\n",
      "numerical_features of text2:\n",
      " [ 0.33333333  0.          0.33333333  0.          0.          0.33333333\n",
      "  0.          0.33333333  0.33333333  0.          0.33333333  0.33333333\n",
      "  0.          0.          0.33333333  0.          0.33333333  0.          0.        ] ; shape: (19,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# merge two texts into one list\n",
    "two_preprocessed_text = [lemmatized_words1, lemmatized_words2]\n",
    "\n",
    "# define tfidf vectorizer\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None)\n",
    "\n",
    "# train\n",
    "model = tfidf.fit(two_preprocessed_text)\n",
    "# transform to numerical features using the trained model\n",
    "numerical_features = model.transform(two_preprocessed_text).toarray()\n",
    "\n",
    "print(\"numerical_features of text1:\\n\", numerical_features[0],\n",
    "      \"; shape:\", numerical_features[0].shape)\n",
    "print(\"numerical_features of text2:\\n\", numerical_features[1],\n",
    "      \"; shape:\", numerical_features[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 01 (Q01)\n",
    "What is/are the difference(s) between stemming and lemmatization?\n",
    "\n",
    "**Answer:**<br>\n",
    "Stemming is reducing words to their root form which does not need to be a valid word on the dictionary. It seek for common morphological words and simply chop off the word to the minimum length of the similar words.\n",
    "\n",
    "Meanwhile lemmatization is reducing words to their dictonary form words which should be registered in the dictonary. It uses lexical knowledge to get the base form of a dictionary word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 02 (Q02)\n",
    "Please explain what TF-IDF is! <br>\n",
    "***Note***: (i) you can insert picture (if you want) in the answer, and then upload all the materials (this ipynb file and the pictures) into one zip file to the course portal, (ii) you can also use mathematical equation here, for exampe: you can write $log_{2}(P_{i})$ by using `$log_{2}(P_{i})$`.\n",
    "\n",
    "**Answer:**<br>\n",
    "**TF-IDF** (term frequency — inverse document frequency) is a method to score relevancy of a term in a document, by checking its occurence rate in a document and the importancy of the term.\n",
    "\n",
    "**TF** (Term Frequency) measures how frequently a term occures in a document. If a word occurs multiple times, this word might has a high term frequency.\n",
    "\n",
    "TF(t) = (Number of times term t appears in the document)/(Total number of terms in the document)\n",
    "\n",
    "**IDF** (Inverse Document Frequency) measures how important a term is. Not every word with high term frequency is important (such as \"is\", \"of\"). So we need to weigh down the frequent terms and scale up the rare ones.\n",
    "\n",
    "IDF(t) = log(Total number of documents/Number of documents with the term t in it)\n",
    "\n",
    "For a term i in document j:\n",
    "\n",
    "$w_{i,j} = tf_{i,j} * log(\\frac{N}{df(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Question 03 (Q03)\n",
    "What are other methods that can be used to convert \"preprocessed text\" to \"numerical features\" other than TF-IDF?\n",
    "\n",
    "**Answer:**<br>\n",
    "**Bag of Words**\n",
    "On bag of words, a text is represented as the bag (multiset) of its words. This method checks whether the word is present in the text or not without considering the order of the words.\n",
    "\n",
    "Example: vector conversion of sentence\n",
    "```\n",
    "    John likes to watch movies. Mary likes movies too. John also likes to watch football games.\n",
    "```\n",
    "\n",
    "can be represented as:\n",
    "\n",
    "```\n",
    "    BoW = {\"John\":2,\"likes\":3,\"to\":2,\"watch\":2,\"movies\":2,\"Mary\":1,\"too\":1,\"also\":1,\"football\":1,\"games\":1};\n",
    "```\n",
    "\n",
    "By using CountVectorizer function, we can convert text document to matrix of word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
