{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W06 - Text Classifier\n",
    "In this hands-on session, we will create a sentiment analyzer on twitter using the concept of classification and text pre-processing that we have learned before. We will cover:<br>\n",
    "a. text pre-processing,<br>\n",
    "b. splitting data for training and testing and converting them into numerical features,<br>\n",
    "c. training a classifier model and perform predictions on testing dataset,<br>\n",
    "d. Evaluating peformance of algorithm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset \"tweets.csv\"\n",
    "Use the dataset of \"tweets.csv\" in zip file provided in the course portal \"IF4041 Ilmu Data dan Penggalian Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1038</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>that film is fantastic #brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>this music is really bad #myband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1693</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>winter is terrible #thumbs-down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1477</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>this game is awful #nightmare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I love jam #loveit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource                      SentimentText\n",
       "0    1038          1    Sentiment140  that film is fantastic #brilliant\n",
       "1    1804          1    Sentiment140   this music is really bad #myband\n",
       "2    1693          0    Sentiment140    winter is terrible #thumbs-down\n",
       "3    1477          0    Sentiment140      this game is awful #nightmare\n",
       "4      45          1    Sentiment140                 I love jam #loveit"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('./dataset/tweets.csv', sep=\",\")# ajust with your own path\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 01 (Q01)\n",
    "The given dataset is still a 'raw dataset' which include some unwanted features, unwanted characters, etc.<br>\n",
    "a. Select the `SentimentText` column as an attribute and the `Sentiment` column as a label (ground truth) for this study case<br><br>\n",
    "b. In this Q01.b, you have been provided a function templete `pre_process` (see below) to perform all the pre-processing step to the all tweet data in the dataset. Complete pre-process function with all techniques that you have learned in the previous hands-on week (W03) for text pre-processing, so the all text attributes can be converted to `pre-processed text`, e.g., after being applied by: (i) tokenization, (ii) normalization, (iii) cleaning, (iv) stemming or lemmatization. Here, you will get `list of words`.<br><br>\n",
    "c. Use the function that you have completed in Q01.b, looped for each data row of `SentimentText` column. For each looping, you will get `list of words`. Append this `list of words` for each looping result in to list, so, will get `list of list`.<br><br>\n",
    "\n",
    "d. Split (random & stratified) `list of list` you get in Q01.c into `training data` and `testing data`. The testing dataset must be 20% from overall dataset. Print the total number of initial dataset, total number of training dataset and testing dataset. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>that film is fantastic #brilliant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>this music is really bad #myband</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>winter is terrible #thumbs-down</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>this game is awful #nightmare</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I love jam #loveit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SentimentText  Sentiment\n",
       "0  that film is fantastic #brilliant          1\n",
       "1   this music is really bad #myband          1\n",
       "2    winter is terrible #thumbs-down          0\n",
       "3      this game is awful #nightmare          0\n",
       "4                 I love jam #loveit          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset = tweets[['SentimentText', 'Sentiment']]\n",
    "input_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def pre_process(input_ori):\n",
    "    '''\n",
    "    Write code implementation for text pre-processing in here. \n",
    "    Use what you have learned in previous meeting about text pre-processing.\n",
    "    \n",
    "    Parameter:\n",
    "    input_ori = raw data text (single tweet data)\n",
    "    \n",
    "    Return value:\n",
    "    processed_tweet = 'list of words'\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # tokenization\n",
    "    tokenized_words = input_ori.split()\n",
    "\n",
    "    # normalization\n",
    "    normalized_words = [w.lower() for w in tokenized_words]\n",
    "\n",
    "    # cleaning\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    punc_removed = [w.translate(table) for w in normalized_words]\n",
    "\n",
    "    isalpha_words = [word for word in punc_removed if word.isalpha()]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words_removed = [w for w in isalpha_words if not w in stop_words]\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in stop_words_removed]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = []\n",
    "for text in input_dataset['SentimentText']:\n",
    "    preprocessed_text.append(pre_process(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dataset 1932\n",
      "Total number of training dataset 1545\n",
      "Total number of testing dataset 387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = np.array(preprocessed_text)\n",
    "y = input_dataset['Sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('Total number of dataset', X.size)\n",
    "print('Total number of training dataset', X_train.size)\n",
    "print('Total number of testing dataset', X_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 02 (Q02)\n",
    "a. Build `tfidf_model` by using function below with `training data` you get in Q01.d. \n",
    "```\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None)\n",
    "```\n",
    "b. Transform `training data` and `testing data` you get in Q01.d by using `tfidf_model` you get in Q02.a. In this case, you will get numerical features, both from `training data` and `testing data`.<br><br>\n",
    "c. Choose one classification algorithm that you will implement in this task, and explain why you choose it.<br><br>\n",
    "d. Train the classifier model based on the algorithm you have chosen by using numerical features of `training data` from Q02.b.<br><br>\n",
    "e. Make predictions on the numerical features of `testing dataset` you get in Q02.b using the classifier model that you have trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "train_model = tfidf.fit(X_train)\n",
    "X_numeric_train = train_model.transform(X_train).toarray()\n",
    "\n",
    "# X_test\n",
    "X_numeric_test = train_model.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your explanation (Q02.c): Dipilih model Random Forest karena menggunakan classifier decision tree dengan ekstensi. Decision tree cocok untuk memodelkan data tfidf kata, karena dilakukan feature selection pada kata yang paling relevan terlebih dahulu (menggunakan nilai dari tfidf per kata). Kemudian digunakan Random Forest untuk mencegah overfitting pada decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)\n",
    "clf = clf.fit(X_numeric_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_numeric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 03 (Q03)\n",
    "After train the classification model and make prediction using that model, now you will evaluate the performance of your model against testing dataset.<br>\n",
    "a. Calculate and print the accuracy of your model's predictions in Q02.e against testing dataset ground truth<br>\n",
    "b. What you can infer based on the result ?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Random Forest:  0.9741602067183462\n"
     ]
    }
   ],
   "source": [
    "print('Score Random Forest: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer (Q03.b) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan menggunakan random forest, dihasilkan score akurasi sebesar 0.9741. Selain pengaruh dari pemilihan model, kualitas dari data latih ini juga baik, sehingga akurasi ketika memakai model lain juga menghasilkan score yang tinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
